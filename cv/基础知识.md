# 人体检测 + 人脸比对



```python
import cv2
import numpy as np
import os
import time
import face_recognition
from mtcnn import MTCNN

single_pair_path = "results/singlePair"
multiplayer_path = "results/multiplayer"
label_path = './cfg/coco.names'
config_path = './cfg/yolov3.cfg'
weights_path = './cfg/yolov3.weights'
# 人脸文件夹
known_images_path = "./test_face/"
net = cv2.dnn.readNetFromDarknet(config_path, weights_path)

from deepface.extendedmodels import Age, Gender, Race, Emotion
from deepface.DeepFace import represent
from deepface.commons import functions, realtime, distance as dst

# 人脸识别
models = ["VGG-Face", "Facenet", "Facenet512", "OpenFace", "DeepFace", "DeepID", "ArcFace", "Dlib", "SFace", ]
# 相似度计算
metrics = ["cosine", "euclidean", "euclidean_l2"]
# 检测器
backends = ['opencv', 'ssd', 'dlib', 'mtcnn', 'retinaface', 'mediapipe', 'yolov8', 'yunet', 'fastmtcnn', ]


# 特征提取
def known_extract_faces(img, model_name, normalization, target_size=(224, 224), detector_backend="opencv",
                        grayscale=False, enforce_detection=True, align=True, ):
    img1_objs = functions.extract_faces(img=img, target_size=target_size, detector_backend=detector_backend,
                                        grayscale=False, enforce_detection=enforce_detection, align=align, )
    for img1_content, img1_region, _ in img1_objs:
        img1_embedding_obj = represent(img_path=img1_content, model_name=model_name,
                                       enforce_detection=enforce_detection,
                                       detector_backend="skip", align=align, normalization=normalization, )
        # 人脸特征
        img1_representation = img1_embedding_obj[0]["embedding"]
        return img1_representation


# 人脸特征
def process_known_images(known_images_path, model_name="VGG-Face",
                         detector_backend="opencv",
                         enforce_detection=False,
                         align=True,
                         normalization="base", ):
    """
        :param known_images_path: 人脸路径
        :return: known_faces:特征集, known_names:标签集
        :param model_name:
        :param detector_backend:
        :param enforce_detection:
        :param align:
        :param normalization:

        """
    known_faces = []
    known_names = []
    for person_folder in os.listdir(known_images_path):
        person_path = os.path.join(known_images_path, person_folder)
        # 对于每个人物的文件夹，处理其中的所有图像
        for image_name in os.listdir(person_path):
            image_path = os.path.join(person_path, image_name)
            target_size = functions.find_target_size(model_name=model_name)
            img1_representation = known_extract_faces(img=image_path, model_name=model_name,
                                                      detector_backend=detector_backend,
                                                      grayscale=False, enforce_detection=enforce_detection, align=align,
                                                      normalization=normalization)
            known_faces.append(img1_representation)
            known_names.append(person_folder)

    return known_faces, known_names


# 人脸比对
def classify_unknown_images(image, model_name="VGG-Face",
                            detector_backend="opencv",
                            distance_metric="cosine",
                            enforce_detection=False,
                            align=True,
                            normalization="base", ):
    tic = time.time()
    img_representation = known_extract_faces(img=image, model_name=model_name, detector_backend=detector_backend,
                                             grayscale=False,
                                             enforce_detection=enforce_detection, align=align,
                                             normalization=normalization)
    print(img_representation)
    distances = []
    # regions = []
    for item, val in enumerate(known_faces):
        if distance_metric == "cosine":
            distance = dst.findCosineDistance(img_representation, val)
        elif distance_metric == "euclidean":
            distance = dst.findEuclideanDistance(img_representation, val)
        elif distance_metric == "euclidean_l2":
            distance = dst.findEuclideanDistance(
                dst.l2_normalize(img_representation), dst.l2_normalize(val)
            )
        else:
            raise ValueError("Invalid distance_metric passed - ", distance_metric)

        distances.append(distance)
    # threshold = dst.findThreshold(model_name, distance_metric)
    # distance = min(distances)
    # print(threshold)
    # print(distances)
    # print(distance)
    # print(distances.index(min(distances)))
    # print("-------------")
    # print(known_names[distances.index(min(distances))])
    # print(image)
    # print("-------------")
    toc = time.time()

    # resp_obj = {
    #     "verified": distance <= threshold,
    #     "": distance,
    #     "threshold": threshold,
    #     "model": model_name,
    #     "detector_backend": detector_backend,
    #     "similarity_metric": distance_metric,
    #
    #     "time": round(toc - tic, 2),
    # }
    # regions.append((img1_region, img2_region))
    return image, known_names[distances.index(min(distances))]


def image_write(path, name, image):
    if not os.path.exists(path):
        os.makedirs(path)
    cv2.imwrite(os.path.join(path, name), image)


known_faces, known_names = process_known_images(
    known_images_path="face_detection/face",
    model_name="ArcFace",
    detector_backend="opencv",
    enforce_detection=False,
    align=True,
    normalization="base"
)


def NMSBoxes_fix(boxes, confidences, confidence_thre, nms_thre, class_id):
    class_id_set = set(class_id)  # 总共有几类
    result = []  # 用来存储结果的
    for cls in class_id_set:  # 遍历每个类别
        cls_boxes = []  # 用来保存每个类别的  边框
        cls_confidences = []  # 用来保存每个类别边框的分数
        indices = [i for i, c in enumerate(class_id) if c == cls]  # 某一类在原始输入的所有索引
        for i in indices:
            cls_boxes.append(boxes[i])  # 把属于该类的框框和分数找出来
            cls_confidences.append(confidences[i])
        idxs = cv2.dnn.NMSBoxes(cls_boxes, cls_confidences, confidence_thre, nms_thre)  # 对每类进行 NMS 操作
        for i in idxs:  # 找出原始输入的索引，并把经过 NMS 操作后保留下来的框框的索引保存下来到一个列表中
            result.append([indices[i]])  #
    return np.array(result)  # opencv 原始的 NMS 输出是一个 np.array 的数据，所以我们也将其转化成指定格式


def file_write(file, path):
    pass


def yolo_detect(pathIn='', label_path=label_path, net=net, confidence_thre=0.80, nms_thre=0.5, jpg_quality=80):
    '''
    :param pathIn：原始图片的路径
    :param pathOut：结果图片的路径
    :param label_path：类别标签文件的路径
    :param config_path：模型配置文件的路径
    :param weights_path：模型权重文件的路径
    :param confidence_thre：0-1，置信度（概率/打分）阈值，即保留概率大于这个值的边界框，默认为0.5
    :param nms_thre：非极大值抑制的阈值，默认为0.3
    :param jpg_quality：设定输出图片的质量，范围为0到100，默认为80，越大质量越好
    :return none
    '''

    # 加载类别标签文件
    LABELS = open(label_path).read().strip().split("\n")
    nclass = len(LABELS)

    # 为每个类别的边界框随机匹配相应颜色
    np.random.seed(42)
    COLORS = np.random.randint(0, 255, size=(nclass, 3), dtype='uint8')
    # 载入图片并获取其维度
    base_path = os.path.basename(pathIn)
    img = cv2.imread(pathIn)
    (H, W) = img.shape[:2]
    # 加载模型配置和权重文件
    # 获取YOLO输出层的名字
    ln = net.getLayerNames()
    ln = [ln[i - 1] for i in net.getUnconnectedOutLayers()]

    # 将图片构建成一个blob，设置图片尺寸，然后执行一次
    # YOLO前馈网络计算，最终获取边界框和相应概率
    blob = cv2.dnn.blobFromImage(img, 1 / 255.0, (416, 416), swapRB=True, crop=False)
    net.setInput(blob)
    start = time.time()
    layerOutputs = net.forward(ln)
    end = time.time()

    # 显示预测所花费时间
    print('YOLO模型花费 {:.2f} 秒来预测一张图片'.format(end - start))

    # 初始化边界框，置信度（概率）以及类别
    boxes = []
    confidences = []
    classIDs = []
    # 迭代每个输出层，总共三个
    for output in layerOutputs:
        # 迭代每个检测
        for detection in output:
            # 提取类别ID和置信度
            scores = detection[5:]
            classID = np.argmax(scores)
            confidence = scores[classID]
            # 只保留置信度大于某值的边界框
            if confidence > confidence_thre:
                # 将边界框的坐标还原至与原图片相匹配，记住YOLO返回的是
                # 边界框的中心坐标以及边界框的宽度和高度
                box = detection[0:4] * np.array([W, H, W, H])
                (centerX, centerY, width, height) = box.astype("int")
                # 计算边界框的左上角位置
                x = int(centerX - (width / 2))
                y = int(centerY - (height / 2))
                # print(LABELS[classID])
                if LABELS[classID] == "person":
                    # 更新边界框，置信度（概率）以及类别
                    boxes.append([x, y, int(width), int(height)])
                    confidences.append(float(confidence))
                    # print(LABELS[classID])
                    classIDs.append(classID)

    # 使用非极大值抑制方法抑制弱、重叠边界框
    # idxs = cv2.dnn.NMSBoxes(boxes, confidences, confidence_thre, nms_thre)
    # 自己修正过的 非极大值抑制方法
    idxs = NMSBoxes_fix(boxes, confidences, confidence_thre, nms_thre, classIDs)
    # 确保至少一个边界框
    if 0 < len(idxs) <= 1:
        # single_pair_path迭代每个边界框
        # if len(idxs) == 1:
        #     index = (idxs.flatten())[0]
        # else:
        #     index = (idxs.flatten())[0] if (idxs.flatten())[0] > (idxs.flatten())[1] else (idxs.flatten())[1]
        # (x, y) = (boxes[index][0], boxes[index][1])
        # (w, h) = (boxes[index][2], boxes[index][3])
        # # 绘制边界框以及在左上角添加类别标签和置信度
        # color = [int(c) for c in COLORS[classIDs[index]]]
        # cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)
        # text = '{}: {:.3f}'.format(LABELS[classIDs[index]], confidences[index])
        # (text_w, text_h), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
        # cv2.rectangle(img, (x, y - text_h - baseline), (x + text_w, y), color, -1)
        # cv2.putText(img, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
        if not os.path.exists(single_pair_path):
            os.makedirs(single_pair_path)
        image, names = classify_unknown_images(
            image=img,
            model_name="ArcFace",
            detector_backend="opencv",
            distance_metric="euclidean_l2",
            enforce_detection=False,
            align=True,
            normalization="base"
        )
        # print(single_pair_path)
        # print(base_path)
        # cv2.imwrite(os.path.join(single_pair_path, known_names, base_path))
        image_write(os.path.join(single_pair_path, names), base_path, image)
        # cv2.imwrite(os.path.join(single_pair_path, base_path), img, [int(cv2.IMWRITE_JPEG_QUALITY), jpg_quality])


    elif 2 <= len(idxs):
        for i in idxs.flatten():
            # 提取边界框的坐标
            (x, y) = (boxes[i][0], boxes[i][1])
            (w, h) = (boxes[i][2], boxes[i][3])
            # 绘制边界框以及在左上角添加类别标签和置信度
            color = [int(c) for c in COLORS[classIDs[i]]]
            cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)
            text = '{}: {:.3f}'.format(LABELS[classIDs[i]], confidences[i])
            # print(confidences[i])
            (text_w, text_h), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
            cv2.rectangle(img, (x, y - text_h - baseline), (x + text_w, y), color, -1)
            cv2.putText(img, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
            if not os.path.exists(multiplayer_path):
                os.makedirs(multiplayer_path)

            cv2.imwrite(os.path.join(multiplayer_path, base_path), img, [int(cv2.IMWRITE_JPEG_QUALITY), jpg_quality])


if __name__ == "__main__":
    '''
        实现对test目录下图片进行推理预测，并将结果保存在results下
    '''
    dir = r'G:\baby'
    for pic in os.listdir(dir):
        pic_path = os.path.join(dir, pic)
        yolo_detect(pic_path, jpg_quality=100)

```

